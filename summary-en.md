This thesis studies adaptive tests within learning environments. It falls within educational data mining and learning analytics, where student educational data is processed so as to optimize their learning.

Computerized assessments allow us to store and analyze student data easily, in order to provide better tests for future learners. In this thesis, we focus on computerized adaptive testing. Such adaptive tests which can ask a question to the learner, analyze their answer on the fly, and choose the next question to ask accordingly. This process reduces the number of questions to ask to a learner while keeping an accurate measurement of their level. Adaptive tests are today massively used in practice, for example in the GMAT and GRE standardized tests, that are administered to hundreds of thousands of students. Traditionally, models used for adaptive assessment have been mostly summative: they measure or rank effectively examinees, but do not provide any other feedback. Recent advances have focused on formative assessments, that provide more useful feedback for both the learner and the teacher; hence, they are more useful for improving student learning.

In this thesis, we have reviewed adaptive testing models from various research communities. We have compared them qualitatively and quantitatively. Thus, we have proposed an experimental protocol that we have implemented in order to compare the most popular adaptive testing models, on real data. This led us to provide a hybrid model for adaptive cognitive diagnosis, better than existing models for formative assessment on all tried datasets. Finally, we have developed a strategy for asking several questions at the beginning of a test in order to measure the learner more accurately. This system can be applied to the automatic generation of worksheets, for example on a massive online open course (MOOC).

We wanted to bring a new, machine-learning point of view for the problem of reducing the size of assessments. In collaborative filtering, one wants to use an active community in order to infer the preferences of a user according to the preferences of other users. In adaptive assessment, one wants to use the history of answers of a test in order to infer the performance of a learner according to the past performances of other learners. We do not intend to do a direct analogy between learning and culture, but to bring new techniques to an existing problem. Cultural distribution platforms are more prepared to receive thousands of users than MOOCs, analyze the data they collect and adapt their content accordingly. Therefore, the algorithms they use do not only rely on strong statistical frameworks but also on an efficient handling of large-scale data.
